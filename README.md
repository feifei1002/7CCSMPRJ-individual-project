# Exploring Prompt Engineering Strategies to Enhance the Capabilities of LLMs in Translating Code  
## Variant 4: Test-Driven Development for Improving LLM-Based Code Translation

**Name**: Fei Liu  
**Student Number**: 24026133  
**Degree Programme**: MSc Advanced Software Engineering  
**Supervisor**: Jie Zhang  

---

## 🧾 Project Summary

This project explores how test-driven development (TDD) can be used as a prompt engineering strategy to enhance the performance of large language models (LLMs) in the context of code translation. Specifically, it investigates the following research questions:

1. **Does Test-Driven Development enhance the capabilities of LLMs in code translation?**  
2. **Do LLM-generated test cases affect LLM performance in code translation?**  
3. **Does collaboration between LLMs in test case generation and code translation improve translation quality?**  
4. **Does the number of test cases influence the LLM’s accuracy in translating code?**  

Through systematic experimentation using both dataset-provided and LLM-generated test cases, this research evaluates different strategies for integrating test-driven prompts and multi-model collaboration.


---

## 📁 Project Structure
````
├── main.py # Main script to run the full pipeline
├── config.py # Core configurations: API keys, language mappings, file extensions
├── llms.py # Configuration and setup for 5 different LLMs
├── data_utils.py # Downloads and extracts dataset code from HumanEval-X using HuggingFace
├── code_execution.py # Logic for executing translated code and test cases
├── code_translator.py # Base class for code translation
├── dataset_translator.py # Translates code using HumanEval-X dataset test cases
├── llm_test_translator.py # Translates code using LLM-generated test cases
├── multi_llm_test_translator.py # Translates code using test cases generated by a different LLM
├── prompts.py # Prompts using dataset-based test cases for translation
├── llms_prompts.py # Prompts using LLM-generated test cases for translation
├── tests_prompts.py # Prompts for generating test cases with LLMs
├── result_analysis.py # Aggregates and stores translation results in tabular form
└── ranking_analysis.py # Visualizes evaluation results using charts and graphs
````


---

## 🔐 LLM API Keys

This project requires API access to multiple large language models. These are the models used in this project:

- **Claude 3.5 Haiku** (Anthropic)
- **Codestral-2501** (MistralAI)
- **DeepSeek-V3** (DeepSeek)
- **GPT-4o-mini** (OpenAI)
- **Gemini-2.0-Flash** (Google)
---

## ✅ Testing Frameworks

This project uses the following testing tools for validating translated code and generated test cases:

- **Python**: `unittest` 
- **JavaScript**: `Jest`  
- **Java**: `JUnit5`

---

## 🧰 Programming Language Versions

The following versions of programming language are used for this project:

- **Python**: 3.10  
- **Java**: 21  
- **JavaScript (Node.js)**: v22

---
