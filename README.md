# Exploring Prompt Engineering Strategies to Enhance the Capabilities of LLMs in Translating Code  
## Variant 4: Test-Driven Development for Improving LLM-Based Code Translation

**Name**: Fei Liu  
**Student Number**: 24026133  
**Degree Programme**: MSc Advanced Software Engineering  
**Supervisor**: Jie Zhang  

---

## ğŸ§¾ Project Summary

This project explores how test-driven development (TDD) can be used as a prompt engineering strategy to enhance the performance of large language models (LLMs) in the context of code translation. Specifically, it investigates the following research questions:

1. **Does Test-Driven Development enhance the capabilities of LLMs in code translation?**  
2. **Do LLM-generated test cases affect LLM performance in code translation?**  
3. **Does collaboration between LLMs in test case generation and code translation improve translation quality?**  
4. **Does the number of test cases influence the LLMâ€™s accuracy in translating code?**  

Through systematic experimentation using both dataset-provided and LLM-generated test cases, this research evaluates different strategies for integrating test-driven prompts and multi-model collaboration.


---

## ğŸ“ Project Structure
````
â”œâ”€â”€ main.py # Main script to run the full pipeline
â”œâ”€â”€ config.py # Core configurations: API keys, language mappings, file extensions
â”œâ”€â”€ llms.py # Configuration and setup for 5 different LLMs
â”œâ”€â”€ data_utils.py # Downloads and extracts dataset code from HumanEval-X using HuggingFace
â”œâ”€â”€ code_execution.py # Logic for executing translated code and test cases
â”œâ”€â”€ code_translator.py # Base class for code translation
â”œâ”€â”€ dataset_translator.py # Translates code using HumanEval-X dataset test cases
â”œâ”€â”€ llm_test_translator.py # Translates code using LLM-generated test cases
â”œâ”€â”€ multi_llm_test_translator.py # Translates code using test cases generated by a different LLM
â”œâ”€â”€ prompts.py # Prompts using dataset-based test cases for translation
â”œâ”€â”€ llms_prompts.py # Prompts using LLM-generated test cases for translation
â”œâ”€â”€ tests_prompts.py # Prompts for generating test cases with LLMs
â”œâ”€â”€ result_analysis.py # Aggregates and stores translation results in tabular form
â””â”€â”€ ranking_analysis.py # Visualizes evaluation results using charts and graphs
````


---

## ğŸ” LLM API Keys

This project requires API access to multiple large language models. These are the models used in this project:

- **Claude 3.5 Haiku** (Anthropic)
- **Codestral-2501** (MistralAI)
- **DeepSeek-V3** (DeepSeek)
- **GPT-4o-mini** (OpenAI)
- **Gemini-2.0-Flash** (Google)
---

## âœ… Testing Frameworks

This project uses the following testing tools for validating translated code and generated test cases:

- **Python**: `unittest` 
- **JavaScript**: `Jest`  
- **Java**: `JUnit5`

---

## ğŸ§° Programming Language Versions

The following versions of programming language are used for this project:

- **Python**: 3.10  
- **Java**: 21  
- **JavaScript (Node.js)**: v22

---
